import type { LLMProvider, ProviderConfig } from "~types"
import { parseSSEStream, type ParsedChunk } from "~services/stream-parser"

/**
 * Custom OpenAI-compatible API Provider
 * æ”¯æŒä»»ä½•å…¼å®¹ OpenAI API çš„æœåŠ¡
 */
export class CustomProvider implements LLMProvider {
  private apiKey: string
  private baseURL: string
  private model: string
  private temperature: number
  private maxTokens: number

  constructor(config: ProviderConfig) {
    this.apiKey = config.apiKey
    this.baseURL = config.baseURL || "https://api.openai.com/v1"
    this.model = config.model || "gpt-4o-mini"
    this.temperature = config.temperature ?? 0.7
    this.maxTokens = config.maxTokens || 2000
  }

  /**
   * è·å–å½“å‰æ¨¡å‹åç§°
   */
  getModel(): string {
    return this.model
  }

  /**
   * è·å– Base URL
   */
  getBaseURL(): string {
    return this.baseURL
  }

  async generateReply(prompt: string): Promise<string> {
    if (!this.apiKey) {
      throw new Error("Custom API Key æœªé…ç½®")
    }

    if (!this.baseURL) {
      throw new Error("Custom Base URL æœªé…ç½®")
    }

    console.log(`ğŸ¤– è°ƒç”¨ Custom API - æ¨¡å‹: ${this.model}, Base URL: ${this.baseURL}`)

    try {
      const response = await fetch(`${this.baseURL}/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.model,
          messages: [
            {
              role: "user",
              content: prompt
            }
          ],
          temperature: this.temperature,
          max_tokens: this.maxTokens
        })
      })

      if (!response.ok) {
        const error = await response.json()
        throw new Error(
          error.error?.message || `Custom API é”™è¯¯: ${response.status}`
        )
      }

      const data = await response.json()
      const content = data.choices[0]?.message?.content

      if (!content) {
        throw new Error("Custom API è¿”å›çš„å†…å®¹ä¸ºç©º")
      }

      console.log(`âœ… Custom API å›å¤æˆåŠŸ - æ¨¡å‹: ${this.model}, é•¿åº¦: ${content.length} å­—ç¬¦`)

      return content.trim()
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`Custom API è°ƒç”¨å¤±è´¥: ${error.message}`)
      }
      throw error
    }
  }

  async testConnection(): Promise<boolean> {
    try {
      await this.generateReply("æµ‹è¯•è¿æ¥")
      return true
    } catch {
      return false
    }
  }

  /**
   * æµå¼ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒ thinking + content åˆ†ç¦»ï¼‰
   * @param messages å¤šè½®å¯¹è¯æ¶ˆæ¯æ•°ç»„
   * @param signal ç”¨äºä¸­æ–­çš„ AbortSignal
   */
  async *generateReplyStream(
    messages: Array<{ role: string; content: string }>,
    signal?: AbortSignal
  ): AsyncGenerator<ParsedChunk> {
    if (!this.apiKey) {
      throw new Error("Custom API Key æœªé…ç½®")
    }

    if (!this.baseURL) {
      throw new Error("Custom Base URL æœªé…ç½®")
    }

    console.log(`ğŸ¤– è°ƒç”¨ Custom API (æµå¼) - æ¨¡å‹: ${this.model}, Base URL: ${this.baseURL}`)

    const response = await fetch(`${this.baseURL}/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: this.model,
        messages,
        temperature: this.temperature,
        max_tokens: this.maxTokens,
        stream: true
      }),
      signal
    })

    if (!response.ok) {
      let errorMessage = `Custom API é”™è¯¯: ${response.status}`
      try {
        const error = await response.json()
        errorMessage = error.error?.message || errorMessage
      } catch {
        // å¿½ç•¥ JSON è§£æé”™è¯¯
      }
      throw new Error(errorMessage)
    }

    if (!response.body) {
      throw new Error("å“åº”ä½“ä¸ºç©º")
    }

    const reader = response.body.getReader()

    try {
      for await (const chunk of parseSSEStream(reader)) {
        yield chunk
      }
      console.log(`âœ… Custom API æµå¼å›å¤å®Œæˆ - æ¨¡å‹: ${this.model}`)
    } finally {
      reader.releaseLock()
    }
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  static async fetchModels(
    baseURL: string,
    apiKey: string
  ): Promise<string[]> {
    try {
      const response = await fetch(`${baseURL}/models`, {
        method: "GET",
        headers: {
          Authorization: `Bearer ${apiKey}`
        }
      })

      if (!response.ok) {
        throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${response.status}`)
      }

      const data = await response.json()
      
      if (!data.data || !Array.isArray(data.data)) {
        throw new Error("è¿”å›çš„æ¨¡å‹æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
      }

      return data.data
        .map((model: any) => model.id)
        .filter((id: string) => id && typeof id === "string")
        .sort()
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${error.message}`)
      }
      throw error
    }
  }
}

