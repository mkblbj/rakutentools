import type { LLMProvider, ProviderConfig } from "~types"

/**
 * Custom OpenAI-compatible API Provider
 * æ”¯æŒä»»ä½•å…¼å®¹ OpenAI API çš„æœåŠ¡
 */
export class CustomProvider implements LLMProvider {
  private apiKey: string
  private baseURL: string
  private model: string
  private temperature: number
  private maxTokens: number

  constructor(config: ProviderConfig) {
    this.apiKey = config.apiKey
    this.baseURL = config.baseURL || "https://api.openai.com/v1"
    this.model = config.model || "gpt-4o-mini"
    this.temperature = config.temperature ?? 0.7
    this.maxTokens = config.maxTokens || 2000
  }

  async generateReply(prompt: string): Promise<string> {
    if (!this.apiKey) {
      throw new Error("Custom API Key æœªé…ç½®")
    }

    if (!this.baseURL) {
      throw new Error("Custom Base URL æœªé…ç½®")
    }

    console.log(`ğŸ¤– è°ƒç”¨ Custom API - æ¨¡å‹: ${this.model}, Base URL: ${this.baseURL}`)

    try {
      const response = await fetch(`${this.baseURL}/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.model,
          messages: [
            {
              role: "user",
              content: prompt
            }
          ],
          temperature: this.temperature,
          max_tokens: this.maxTokens
        })
      })

      if (!response.ok) {
        const error = await response.json()
        throw new Error(
          error.error?.message || `Custom API é”™è¯¯: ${response.status}`
        )
      }

      const data = await response.json()
      const content = data.choices[0]?.message?.content

      if (!content) {
        throw new Error("Custom API è¿”å›çš„å†…å®¹ä¸ºç©º")
      }

      console.log(`âœ… Custom API å›å¤æˆåŠŸ - æ¨¡å‹: ${this.model}, é•¿åº¦: ${content.length} å­—ç¬¦`)

      return content.trim()
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`Custom API è°ƒç”¨å¤±è´¥: ${error.message}`)
      }
      throw error
    }
  }

  async testConnection(): Promise<boolean> {
    try {
      await this.generateReply("æµ‹è¯•è¿æ¥")
      return true
    } catch {
      return false
    }
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  static async fetchModels(
    baseURL: string,
    apiKey: string
  ): Promise<string[]> {
    try {
      const response = await fetch(`${baseURL}/models`, {
        method: "GET",
        headers: {
          Authorization: `Bearer ${apiKey}`
        }
      })

      if (!response.ok) {
        throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${response.status}`)
      }

      const data = await response.json()
      
      if (!data.data || !Array.isArray(data.data)) {
        throw new Error("è¿”å›çš„æ¨¡å‹æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
      }

      return data.data
        .map((model: any) => model.id)
        .filter((id: string) => id && typeof id === "string")
        .sort()
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: ${error.message}`)
      }
      throw error
    }
  }
}

